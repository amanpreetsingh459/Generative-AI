{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f54f39-d32a-4173-a35f-cfd6581d7617",
   "metadata": {},
   "source": [
    "# Exercise: Self-attention mechanism in GPT Transformer\n",
    "\n",
    "In this exercise you will be implementing the `forward()` function of the `MultiHeadSelfAttention` module in a minified GPT implementation. GPT refers to the \"Generative Pre-trained Transformers\" paper from OpenAI, originally described in [this paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) [1].\n",
    "\n",
    "A full GPT model consists of a sequence of GPT Blocks (transformer). The self-attention module is invoked in the GPT Block, where the input sequence of tokens are layer-normalized before application and added to itself constituting a residual connection. For extra reading on residual connections and why they are useful to avoid gradient collapse, see [this image](https://miro.medium.com/v2/resize:fit:640/format:webp/1*mxJ5gBvZnYPVo0ISZE5XkA.png) [3] and the [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf) [2]. \n",
    "\n",
    "```\n",
    "def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "```\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding with unsupervised learning. Technical report, OpenAI (2018).\n",
    "\n",
    "[2] He, K., Zhang, X., Ren, S., & Sun, J. (2015). Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n",
    "\n",
    "[3] https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484\n",
    "\n",
    "## Scaled Multiplicative Attention\n",
    "\n",
    "Recall this attention formula:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "This is represented in the Python code as follows:\n",
    "\n",
    "* $Q$: `q`\n",
    "* $K$: `k`\n",
    "* $V$: `v`\n",
    "* $\\text{softmax}$: `F.softmax()`\n",
    "* $K^T$: `k_t`\n",
    "* $QK^T$ (matrix multiplication): `q @ k_t`\n",
    "* $\\sqrt{}$: `math.sqrt()`\n",
    "* $d_k$: `d_k`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f83f7f",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "Within the `MultiHeadSelfAttention` class, fill in the TODO portion of the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed032feb-890b-4cf9-97ae-100938a360fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    cross_attention = False\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "            \n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # START OF SOLUTION\n",
    "        # multiply q and k_t matrices, then divide by the square root of d_k\n",
    "        k_t = k.transpose(-2, -1)\n",
    "        d_k = k.size(-1)\n",
    "        att = q @ k_t / math.sqrt(d_k)\n",
    "\n",
    "        # set the mask fill value to negative infinity\n",
    "        masked_fill_value = float('-inf')\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, masked_fill_value)\n",
    "\n",
    "        # apply softmax and regularization\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        # multiply att and v matrices\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        # END OF SOLUTION\n",
    "\n",
    "        # re-assemble all head outputs side by side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) \n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c036f69e-77cf-49b9-9c6d-cd2ecd449642",
   "metadata": {},
   "source": [
    "## GPT model definition\n",
    "\n",
    "- the initial stem consists of a combination of token encoding and a positional encoding\n",
    "- the meat of it is a uniform sequence of Transformer blocks\n",
    "    - each Transformer is a sequential combination of a 1-hidden-layer MLP block and a self-attention block\n",
    "    - all blocks feed into a central residual pathway similar to resnets\n",
    "- the final decoder is a linear projection into a Softmax classifier\n",
    "\n",
    "Run this cell without changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d2f2605-b960-4ef0-9917-f44e1ee24894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        if config.cross_attention:\n",
    "            self.attn = CrossAttention(config)\n",
    "        else:\n",
    "            self.attn = MultiHeadSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # input embedding stem\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        print(\"number of parameters: {}\".format(sum(p.numel() for p in self.parameters())))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        # forward the GPT model\n",
    "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=0)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ff8012-95d6-4000-b526-952f6c275bc7",
   "metadata": {},
   "source": [
    "# Training the GPT model\n",
    "\n",
    "We will now \"pretrain\" our GPT transformer on Wikipedia to predict a notable person's birth place, using the **self-supervised pretext objective of next token prediction** to generate one character at a time. By exposing the transformer to world knowledge, this enables it to perform considerably above chance. \n",
    "\n",
    "You can read the accompanied `dataset.py` code for details on how the training data is prepared, which follows the span corruption denoising objective whereby it randomly selects spans of text in a document and replaces them with `MASK` tokens, as outlined in the [T5 paper](https://arxiv.org/pdf/1910.10683.pdf) [4]. \n",
    "\n",
    "Example pretraining data (x, y) for next token prediction. `⁇` denotes the masked span/text and `□` denotes padding tokens:\n",
    "\n",
    "```\n",
    "x: Khatchig Mouradian. Khatchig Mouradian is a journalist, writer and translator bo⁇non .⁇rn in Leba□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
    "y: hatchig Mouradian. Khatchig Mouradian is a journalist, writer and translator bo⁇non .⁇rn in Leba□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
    "```\n",
    "\n",
    "### References\n",
    "[4] Raffel, C., Shazeer, N.M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P.J. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res., 21, 140:1-140:67.\n",
    "\n",
    "Run this cell without changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87f2c1cd-e324-4d93-8f19-443e9a971b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A multiprocessing context has been set.\n",
      "data has 418352 characters, 256 unique.\n"
     ]
    }
   ],
   "source": [
    "import dataset\n",
    "import trainer\n",
    "\n",
    "import multiprocess as mp\n",
    "if mp.get_start_method() is None:\n",
    "    mp.set_start_method(\"fork\")\n",
    "else:\n",
    "    print(\"A multiprocessing context has been set.\")\n",
    "\n",
    "BLOCK_SIZE = 128\n",
    "PRETRAIN_CORPUS = 'wiki.txt'\n",
    "SAVE_CKPT_PATH = 'pretrain.params'\n",
    "\n",
    "text = open(PRETRAIN_CORPUS).read()\n",
    "pretrain_dataset = dataset.CharCorruptionDataset(text, BLOCK_SIZE)\n",
    "\n",
    "model_config = GPTConfig(pretrain_dataset.vocab_size, pretrain_dataset.block_size,\n",
    "    n_layer=4, n_head=8, n_embd=256)\n",
    "\n",
    "train_config = trainer.TrainerConfig(max_epochs=1, \n",
    "                                     batch_size=16, \n",
    "                                     learning_rate=6e-4, \n",
    "                                     lr_decay=True, \n",
    "                                     warmup_tokens=512*20, \n",
    "                                     final_tokens=200*len(pretrain_dataset)*BLOCK_SIZE,\n",
    "                                     num_workers=1, \n",
    "                                     ckpt_path=SAVE_CKPT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c7c23-6599-44a4-8702-23230eed1098",
   "metadata": {},
   "source": [
    "In one epoch (a few minutes), your loss should decrease from ~5 to 2.5±0.2 range.\n",
    "\n",
    "Run this cell without changes. It should take 183 iterations to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "006fafd7-abf6-4111-aec7-42d64de4b407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3323392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 183: train loss 2.56776. lr 5.999655e-04: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184/184 [00:22<00:00,  8.17it/s]\n"
     ]
    }
   ],
   "source": [
    "model = GPT(model_config)\n",
    "gpt_trainer = trainer.Trainer(model, pretrain_dataset, None, train_config)\n",
    "gpt_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1c840-f9e4-4ac1-b933-24acbdd748e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
