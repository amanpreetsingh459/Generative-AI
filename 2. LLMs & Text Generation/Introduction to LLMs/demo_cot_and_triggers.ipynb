{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvBHYcpd4v_1"
   },
   "source": [
    "# System setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8pzPvghP5TBs"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "TOGETHER_API_KEY = \"<api key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://api.together.xyz/inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8IF2Fa6tg_w1"
   },
   "outputs": [],
   "source": [
    "# Decoding parameters\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS = 512\n",
    "TOP_P = 1.0\n",
    "REPITIION_PENALTY = 1.0\n",
    "\n",
    "# https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gt80J3Zy5rM4"
   },
   "outputs": [],
   "source": [
    "def query_together_endpoint(prompt):\n",
    "    return requests.post(ENDPOINT, json={\n",
    "        \"model\": \"togethercomputer/llama-2-7b-chat\",\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"prompt\": prompt,\n",
    "        \"request_type\": \"language-model-inference\",\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"repetition_penalty\": REPITIION_PENALTY,\n",
    "        \"stop\": [\n",
    "            E_INST,\n",
    "            E_SYS\n",
    "        ],\n",
    "        \"negative_prompt\": \"\",\n",
    "    }, headers={\n",
    "        \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\",\n",
    "    }).json()['output']['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXJZqpiV1_Pp"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p-piCAwug9fN"
   },
   "outputs": [],
   "source": [
    "def query_model(prompt,  trigger = None, verbose=True, **kwargs):\n",
    "    inst_prompt = f\"{B_INST} {prompt} {E_INST}\"\n",
    "    if trigger:\n",
    "        inst_prompt = inst_prompt + trigger\n",
    "    generation = query_together_endpoint(inst_prompt)\n",
    "    if verbose:\n",
    "        print(f\"*** Prompt ***\\n{inst_prompt}\")\n",
    "        print(f\"*** Generation ***\\n{generation}\")\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZhFzjfQ2CAg"
   },
   "source": [
    "## System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h8w88wHjt5X2"
   },
   "outputs": [],
   "source": [
    "ANSWER_STAGE = \"Provide the direct answer to the user question.\"\n",
    "REASONING_STAGE = \"Describe the step by step reasoning to find the answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e2Oxy5RTs20Z"
   },
   "outputs": [],
   "source": [
    "# System prompt can be constructed in two ways:\n",
    "# 1) Answering the question first or\n",
    "# 2) Providing the reasoning first\n",
    "\n",
    "# Similar ablation performed in \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"\n",
    "# https://arxiv.org/pdf/2201.11903.pdf\n",
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"{b_sys}Answer the user's question using the following format:\n",
    "1) {stage_1}\n",
    "2) {stage_2}{e_sys}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain of thought trigger from \"Large Language Models are Zero-Shot Reasoners\"\n",
    "# https://arxiv.org/abs/2205.11916\n",
    "COT_TRIGGER = \"\\n\\nA: Lets think step by step:\"\n",
    "A_TRIGGER = \"\\n\\nA:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT7pJzdi2M-8"
   },
   "source": [
    "## User prompt for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "iEUcXYNckT6d"
   },
   "outputs": [],
   "source": [
    "user_prompt_template = \"Q: Llama 2 has a context window of {atten_window} tokens. \\\n",
    "If we are reserving {max_token} of them for the LLM response, \\\n",
    "the system prompt uses {sys_prompt_len}, \\\n",
    "the chain of thought trigger uses only {trigger_len}, \\\n",
    "and finally the conversational history uses {convo_history_len}, \\\n",
    "how many can we use for the user prompt?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "atten_window = 4096\n",
    "max_token = 512\n",
    "sys_prompt_len = 124\n",
    "trigger_len = 11\n",
    "convo_history_len = 390\n",
    "\n",
    "user_prompt = user_prompt_template.format(\n",
    "    atten_window=atten_window,\n",
    "    max_token=max_token,\n",
    "    sys_prompt_len=sys_prompt_len,\n",
    "    trigger_len=trigger_len,\n",
    "    convo_history_len=convo_history_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYozeQNor7fd",
    "outputId": "240f1fc1-fb29-4ec8-abd5-1d233845746d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3059"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_numeric_answer = atten_window - max_token - sys_prompt_len - trigger_len - convo_history_len\n",
    "desired_numeric_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7rs_lWP2VWF"
   },
   "source": [
    "## Testing the prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTOKsW82IIxP",
    "outputId": "2e918314-58d9-40b4-f5f0-f02fe9e00817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt ***\n",
      "[INST] Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt? [/INST]\n",
      "*** Generation ***\n",
      "  Great, let's do the calculation!\n",
      "\n",
      "You've mentioned that Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, that means we have 4096 - 512 = 3584 tokens available for other uses.\n",
      "\n",
      "You've also mentioned that the system prompt uses 124 tokens, the chain of thought trigger uses 11 tokens, and the conversational history uses 390 tokens.\n",
      "\n",
      "So, the remaining tokens available for the user prompt are:\n",
      "\n",
      "3584 - 124 - 11 - 390 = 2089 tokens\n",
      "\n",
      "Therefore, you can use up to 2089 tokens for the user prompt.\n"
     ]
    }
   ],
   "source": [
    "r = query_model(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + system prompt v1: answering first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmkqUpP7J5Zw",
    "outputId": "681caacf-c691-4764-f7b0-d27e765ab72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt ***\n",
      "[INST] <<SYS>>\n",
      "Answer the user's question using the following format:\n",
      "1) Provide the direct answer to the user question.\n",
      "2) Describe the step by step reasoning to find the answer.\n",
      "<</SYS>>\n",
      "\n",
      "Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt? [/INST]\n",
      "*** Generation ***\n",
      "  Sure, I'd be happy to help you with that! Here's the answer to your question:\n",
      "\n",
      "1. Direct answer: The user can use up to 390 tokens for their prompt.\n",
      "\n",
      "Here's the reasoning behind this answer:\n",
      "\n",
      "* The context window for Llama 2 is 4096 tokens.\n",
      "* You mentioned that 512 tokens are reserved for the LLM response.\n",
      "* The system prompt uses 124 tokens.\n",
      "* The chain of thought trigger uses only 11 tokens.\n",
      "* The conversational history uses 390 tokens.\n",
      "\n",
      "So, the remaining tokens available for the user prompt are 4096 - 512 - 124 - 11 = 390 tokens.\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = SYSTEM_PROMPT_TEMPLATE.format(\n",
    "    b_sys = B_SYS,\n",
    "    stage_1=ANSWER_STAGE,\n",
    "    stage_2=REASONING_STAGE,\n",
    "    e_sys=E_SYS\n",
    ")\n",
    "prompt = \"\".join([system_prompt, user_prompt])\n",
    "\n",
    "r2 = query_model(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + system prompt v2: reasoning first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfPHZ9v-tnPn",
    "outputId": "bfeac801-a82b-430f-a700-accd443ca775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt ***\n",
      "[INST] <<SYS>>\n",
      "Answer the user's question using the following format:\n",
      "1) Describe the step by step reasoning to find the answer.\n",
      "2) Provide the direct answer to the user question.\n",
      "<</SYS>>\n",
      "\n",
      "Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt? [/INST]\n",
      "*** Generation ***\n",
      "  Great, let's break down the calculation to find out how many tokens are available for the user prompt:\n",
      "\n",
      "1. First, let's calculate the total number of tokens reserved for the LLM response, chain of thought trigger, and conversational history:\n",
      "\n",
      "LLM response: 512 tokens\n",
      "Chain of thought trigger: 11 tokens\n",
      "Conversational history: 390 tokens\n",
      "Total reserved tokens: 512 + 11 + 390 = 903 tokens\n",
      "\n",
      "2. Now, let's subtract the total reserved tokens from the context window of 4096 tokens to find out how many tokens are available for the user prompt:\n",
      "\n",
      "4096 - 903 = 3193 tokens\n",
      "\n",
      "Therefore, the user can use up to 3193 tokens for their prompt.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = SYSTEM_PROMPT_TEMPLATE.format(b_sys = B_SYS, stage_1=REASONING_STAGE, stage_2=ANSWER_STAGE, e_sys=E_SYS)\n",
    "prompt = \"\".join([system_prompt, user_prompt])\n",
    "\n",
    "r3 = query_model(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3059"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3584 - (124 + 11 + 390)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + cot trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt ***\n",
      "[INST] Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt? [/INST]\n",
      "\n",
      "A: Lets think step by step:\n",
      "*** Generation ***\n",
      "\n",
      "\n",
      "1. The context window of Llama 2 is 4096 tokens.\n",
      "2. You want to reserve 512 tokens for the LLM response.\n",
      "3. The system prompt uses 124 tokens.\n",
      "4. The chain of thought trigger uses only 11 tokens.\n",
      "5. The conversational history uses 390 tokens.\n",
      "\n",
      "Now, let's calculate how many tokens are left for the user prompt:\n",
      "\n",
      "4096 - 512 = 3584\n",
      "\n",
      "So, you have 3584 tokens available for the user prompt.\n"
     ]
    }
   ],
   "source": [
    "r4 = query_model(user_prompt, trigger=COT_TRIGGER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + \"A:\" trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt ***\n",
      "[INST] Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt? [/INST]\n",
      "\n",
      "A:\n",
      "*** Generation ***\n",
      " To determine how many context tokens are available for the user prompt, we need to subtract the number of tokens reserved for the LLM response, the system prompt, the chain of thought trigger, and the conversational history from the total context window of 4096 tokens.\n",
      "\n",
      "Reserved tokens for LLM response: 512\n",
      "Reserved tokens for system prompt: 124\n",
      "Reserved tokens for chain of thought trigger: 11\n",
      "Reserved tokens for conversational history: 390\n",
      "\n",
      "Total reserved tokens: 1037\n",
      "\n",
      "Now, let's check how many tokens are available for the user prompt:\n",
      "\n",
      "4096 - 1037 = 3059\n",
      "\n",
      "So, there are 3059 context tokens available for the user prompt.\n"
     ]
    }
   ],
   "source": [
    "r5 = query_model(user_prompt, trigger=A_TRIGGER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt ***\n",
      "[INST] Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt? [/INST]Ans:\n",
      "*** Generation ***\n",
      " To determine how many context tokens are available for the user prompt, we need to subtract the number of tokens reserved for the LLM response, the system prompt, the chain of thought trigger, and the conversational history from the total context window of 4096 tokens.\n",
      "\n",
      "Reserved tokens for LLM response: 512\n",
      "Reserved tokens for system prompt: 124\n",
      "Reserved tokens for chain of thought trigger: 11\n",
      "Reserved tokens for conversational history: 390\n",
      "\n",
      "Total reserved tokens: 1037\n",
      "\n",
      "Now, let's check how many tokens are available for the user prompt:\n",
      "\n",
      "4096 - 1037 = 3059\n",
      "\n",
      "So, there are 3059 context tokens available for the user prompt.\n"
     ]
    }
   ],
   "source": [
    "r5 = query_model(user_prompt, trigger=\"Ans:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3059"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096 - 1037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOiW36Ll4W/LJq40/BjGEnk",
   "include_colab_link": true,
   "mount_file_id": "1SkBFwV9AhTt8ymXpNk2b-7ehiq-TxEb4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
